{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "from jieba import lcut\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pkuseg\n",
    "import thulac  \n",
    "from gensim import corpora\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import numpy as np\n",
    "import string\n",
    "pd.set_option('display.max_rows', 50000)\n",
    "\n",
    "random_seed = 751031\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "#function zone\n",
    "pku_seg = pkuseg.pkuseg()    \n",
    "thu_lac = thulac.thulac(seg_only=True)\n",
    "def Bulid_dict(message_list):\n",
    "# 總分詞\n",
    "    ls_of_words = [pku_seg.cut(text) for text in message_list]\n",
    "    #ls_of_words = [thu_lac.cut(i, text=True) for i in message_list]\n",
    "    rs = []\n",
    "    for j in tqdm(range(len(ls_of_words))):\n",
    "        temp = []\n",
    "        for k in range(len(ls_of_words[j])):\n",
    "            temp.append(remove_punctuation(ls_of_words[j][k]))\n",
    "        rs.append(temp)    \n",
    "    \n",
    "    # 構造詞典\n",
    "    dictionary = corpora.Dictionary(rs)\n",
    "    dt = dictionary.token2id\n",
    "    length = len(dt)\n",
    "    return dt, length\n",
    "\n",
    "def vectorize(ls_of_words, dt):\n",
    "    length = len(dt)\n",
    "    # 句向量\n",
    "    ls_of_wid = []\n",
    "    for words in ls_of_words:#ls_of_words \n",
    "        vector = [0] * length\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector[dt[word]] += 1\n",
    "            except:\n",
    "                continue\n",
    "        ls_of_wid.append(vector) \n",
    "    return ls_of_wid\n",
    "\n",
    "def add_feature(sr):\n",
    "    num_only = int(sr.isdigit())\n",
    "    abc_only = int(sr.encode( 'UTF-8').isalpha())\n",
    "    #numorabc_only = int(sr.isalnum())\n",
    "    def is_contains_chinese(strs):\n",
    "        for _char in strs:\n",
    "            if '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                return 1\n",
    "        return 0\n",
    " \n",
    "    #检验是否全是中文字符\n",
    "    def is_all_chinese(strs):\n",
    "        for _char in strs:\n",
    "            if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                return 0\n",
    "        return 1\n",
    "    #allchinese = is_all_chinese(sr)\n",
    "    #containchinese = is_all_chinese(sr)   \n",
    "    return [num_only, abc_only]\n",
    "\n",
    "\n",
    "def remove_punctuation(s): \n",
    "    punctuation = '''''。：，。？表情\\n!！()-[]{};:'\"\\,<>./[email protected]#$%^&*_~ '''+ string.digits+string.whitespace+ string.punctuation\n",
    "    #'''''\\n!()-[]{};:'\"\\,<>./[email protected]#$%^&*_~！（）－［］｛｝；：’＂＼，＜＞．／＃＄％︿＆＊＿～'''\n",
    "    my_str = s \n",
    "    no_punct = \"\" \n",
    "    for char in my_str: \n",
    "        if char not in punctuation: \n",
    "            no_punct += char \n",
    "    return(no_punct) \n",
    "\n",
    "def change_string(articut, text):\n",
    "    if text == '':\n",
    "        tmp = ''\n",
    "    else:\n",
    "        result = articut.parse(text)\n",
    "        content = articut.getContentWordLIST(result)\n",
    "        tmp = ''\n",
    "        for sentense in content:\n",
    "            for word in sentense:\n",
    "                tmp += word[-1]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構一個總辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify = pd.read_excel(\"C://Users//user//Desktop//project//up//input//input_v5.xlsx\", index_col = None, engine='openpyxl', header=0)\n",
    "data_modify = data_modify[['message', '正确意图']]\n",
    "data_modify = data_modify[~data_modify.loc[:, '正确意图'].isna()]\n",
    "data_modify.columns = ['message', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4675, 2)\n"
     ]
    }
   ],
   "source": [
    "change_dict = {'到帐问题':0, '提款卖币问题':1, '存款买币问题':2, '银行卡问题':3, '操作问题':4, '订单取消问题':5, '非业务意图':6}\n",
    "change_dict_verse = {v: k for k, v in change_dict.items()}\n",
    "data_modify = data_modify[data_modify.label.isin(list(change_dict.keys()))].reset_index(drop=True)\n",
    "print(data_modify.shape)\n",
    "#data_modify.loc[:, 'message'] = data_modify.loc[:, 'message'].apply(remove_punctuation)\n",
    "data_modify.loc[:, 'message'] = [i.upper()  for i in data_modify.message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "model_name = 'uer/roberta-base-finetuned-chinanews-chinese'#'imxly/sentence_roberta_wwm_ext'#'imxly/sentence_roberta_wwm_ext'#\n",
    "#model_name ='sentence-transformers/paraphrase-xlm-r-multilingual-v1'\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "sbert_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(t, model=sbert_model):\n",
    "    result = model.encode(t)\n",
    "    result = result / np.linalg.norm(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_bert = np.zeros((0, 768), float)\\nfor j in tqdm(data_modify.message):\\n    j = remove_punctuation(j)\\n    train_bert = np.append(train_bert, embedding(j).reshape(1,768), axis=0)\\n    \\nnp.save('train_bert.npy', train_bert)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_bert = np.zeros((0, 768), float)\n",
    "for j in tqdm(data_modify.message):\n",
    "    j = remove_punctuation(j)\n",
    "    train_bert = np.append(train_bert, embedding(j).reshape(1,768), axis=0)\n",
    "    \n",
    "np.save('train_bert.npy', train_bert)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert = np.load('train_bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4675/4675 [00:00<00:00, 361451.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#save json and load json file\\nwith open('C://Users//user//Desktop//aicode-master//textLearn//data//up_dict.json', 'w') as fp:\\n    json.dump(dt, fp)\\n\\nwith open('C://Users//user//Desktop//aicode-master//textLearn//data//up_dict.json') as json_file: \\n    dt = json.load(json_file) \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt, length = Bulid_dict(data_modify.message)\n",
    "'''\n",
    "#save json and load json file\n",
    "with open('C://Users//user//Desktop//aicode-master//textLearn//data//up_dict.json', 'w') as fp:\n",
    "    json.dump(dt, fp)\n",
    "\n",
    "with open('C://Users//user//Desktop//aicode-master//textLearn//data//up_dict.json') as json_file: \n",
    "    dt = json.load(json_file) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef is_contains_chinese(strs):\\n    for _char in strs:\\n        if '一' <= _char <= '龥':\\n            return True\\n    return False\\nlook = []\\nfor i in list(dt.keys()):\\n    if ('US' not in i) & (~is_contains_chinese(i)):\\n        look.append(i)\\n        del dt[i]\\n        \\nx = 0\\nfor i in list(dt.keys()):\\n    dt[i] = x\\n    x += 1\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def is_contains_chinese(strs):\n",
    "    for _char in strs:\n",
    "        if '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return True\n",
    "    return False\n",
    "look = []\n",
    "for i in list(dt.keys()):\n",
    "    if ('US' not in i) & (~is_contains_chinese(i)):\n",
    "        look.append(i)\n",
    "        del dt[i]\n",
    "        \n",
    "x = 0\n",
    "for i in list(dt.keys()):\n",
    "    dt[i] = x\n",
    "    x += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVector = CountVectorizer()\n",
    "CountVectorizer_fit = CountVector.fit_transform(data_modify.message)\n",
    "tv = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "tv.fit(data_modify.message)\n",
    "tv_fit = tv.transform(data_modify.message)\n",
    "#pickle.dump(tv, open('tv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [pku_seg.cut(text) for text in data_modify.message]#[lcut(text) for text in data_modify.message]\n",
    "train_list_vec = vectorize(train_list, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify['char_count'] = data_modify['message'].apply(len)\n",
    "data_modify['word_count'] = data_modify['message'].apply(lambda x: len(x.split()))\n",
    "data_modify['word_density'] = data_modify['char_count'] / (data_modify['word_count']+1)\n",
    "data_modify['punctuation_count'] = data_modify['message'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "\n",
    "add_feature_ = data_modify[['word_count', 'word_density', 'punctuation_count']].values\n",
    "add_feature_isin = np.array([add_feature(i) for i in data_modify.message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[np.array(train_list_vec), \n",
    "                   add_feature_isin, \n",
    "                   tv_fit.toarray(),\n",
    "                   add_feature_]\n",
    "maxminscaler = MinMaxScaler()\n",
    "maxminscaler.fit(train_data)\n",
    "train_data = maxminscaler.transform(train_data)\n",
    "#train_data = np.c_[train_data, \n",
    "#                   train_bert]\n",
    "#\n",
    "label_change = [change_dict[i] for i in data_modify.label]\n",
    "data_modify.loc[:, 'label'] = label_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(maxminscaler, open('maxminscaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料切分\n",
    "train_y, test_y, train_x, test_x = train_test_split(data_modify.label, \n",
    "                                                    train_data,#train_list_vec, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=random_seed)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "del(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_oversameple = np.zeros((0,train_x.shape[1]), float)\n",
    "train_y_oversameple = np.zeros((0,), float)\n",
    "\n",
    "max_sample = train_y.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "np.random.seed(random_seed)\n",
    "for i in train_y.unique():\n",
    "    ta = list(np.random.choice(list(train_y[train_y==i].index), max_sample))\n",
    "    train_y_oversameple = np.append(train_y_oversameple, train_y[ta], axis=0)\n",
    "    train_x_oversameple = np.append(train_x_oversameple, train_x[ta], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_fold cross valid accuracy mean:0.6831444299146192, max:0.6983372921615202, min:0.6706302021403091\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "kf = KFold(n_splits=n_fold, random_state=random_seed, shuffle=True)\n",
    "accuracy_list = []\n",
    "for train_index, valid_index in kf.split(train_x):\n",
    "    #print(valid_index)\n",
    "    X_train, X_test = train_x[train_index], train_x[valid_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[valid_index]\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    max_sample = X_train.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "    X_train_oversameple = np.zeros((0, X_train.shape[1]), float)\n",
    "    y_train_oversameple = np.zeros((0,), float)\n",
    "    np.random.seed(random_seed)\n",
    "    for i in range(7):\n",
    "        ta = list(np.random.choice(list(y_train[y_train==i].index), max_sample))\n",
    "        y_train_oversameple = np.append(y_train_oversameple, y_train[ta], axis=0)\n",
    "        X_train_oversameple = np.append(X_train_oversameple, X_train[ta], axis=0)    \n",
    "    classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "    classifier.fit(X_train_oversameple, y_train_oversameple)\n",
    "    # 模型測評\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    accuracy_list.append(score)\n",
    "    #del(X_train, X_test, y_train, y_test, classifier)\n",
    "    \n",
    "print('{k}_fold cross valid accuracy mean:{mean}, max:{max}, min:{min}'.format(k=n_fold, \n",
    "                                                                               mean=mean(accuracy_list),\n",
    "                                                                               max=max(accuracy_list),\n",
    "                                                                               min=min(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy：70.73%\n"
     ]
    }
   ],
   "source": [
    "# 貝葉斯模型訓練\n",
    "classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "classifier.fit(train_x_oversameple, train_y_oversameple)\n",
    "\n",
    "# 模型測評\n",
    "score = classifier.score(test_x, test_y)\n",
    "print('Test set Accuracy：'+ str(round(score*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6578)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#save model and load the model\\nfilename = 'C://Users//user//Desktop//aicode-master//textLearn//models//nb//nb.sav'\\npickle.dump(classifier, open(filename, 'wb'))\\n#classifier = pickle.load(open(filename, 'rb'))\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//aicode-master//textLearn//models//nb//nb.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "5_fold cross valid accuracy mean:0.8288549148310602, max:0.8408551068883611, min:0.8204518430439952\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "random_seed = 751031\n",
    "kf = KFold(n_splits=n_fold, random_state=random_seed, shuffle=True)\n",
    "accuracy_list = []\n",
    "for train_index, valid_index in kf.split(train_x):\n",
    "    X_train, X_test = train_x[train_index], train_x[valid_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[valid_index]\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    #for i in range(7):\n",
    "        #x = list(y_test).count(i)\n",
    "        #ratio = x/ y_test.shape[0]\n",
    "        #print(change_dict_verse[i]+ str(round(ratio*100,2))+ '%')\n",
    "    #print(\"----------------------------\")\n",
    "    max_sample = X_train.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "    X_train_oversameple = np.zeros((0, X_train.shape[1]), float)\n",
    "    y_train_oversameple = np.zeros((0,), float)\n",
    "    np.random.seed(random_seed)\n",
    "    for i in range(7):\n",
    "        ta = list(np.random.choice(list(y_train[y_train==i].index), max_sample))\n",
    "        y_train_oversameple = np.append(y_train_oversameple, y_train[ta], axis=0)\n",
    "        X_train_oversameple = np.append(X_train_oversameple, X_train[ta], axis=0)    \n",
    "    lr = LogisticRegression(solver='saga',\n",
    "                            multi_class='multinomial',\n",
    "                            C=1,\n",
    "                            penalty='l1',\n",
    "                            fit_intercept=True,\n",
    "                            max_iter=50000,\n",
    "                            random_state=4180,\n",
    "                            class_weight='balanced',\n",
    "                            n_jobs=-1)\n",
    "    lr.fit(csr_matrix((X_train_oversameple), dtype=float), \n",
    "           y_train_oversameple)\n",
    "    # 模型測評\n",
    "    score = lr.score(csr_matrix((X_test), dtype=float), \n",
    "                     y_test)\n",
    "    accuracy_list.append(score)\n",
    "    print('-----')    \n",
    "print('{k}_fold cross valid accuracy mean:{mean}, max:{max}, min:{min}'.format(k=n_fold, \n",
    "                                                                               mean=mean(accuracy_list),\n",
    "                                                                               max=max(accuracy_list),\n",
    "                                                                               min=min(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23562, 6578)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oversameple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR - Test set Accuracy：83.12%\n",
      "90.62537479400635\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "lr = LogisticRegression(solver='saga',\n",
    "                        multi_class='multinomial',\n",
    "                        C=1,\n",
    "                        penalty='l1',\n",
    "                        fit_intercept=True,\n",
    "                        max_iter=50000,\n",
    "                        random_state=4180,\n",
    "                        class_weight='balanced',\n",
    "                        n_jobs=-1)\n",
    "lr.fit(csr_matrix((train_x_oversameple), dtype=float), train_y_oversameple)\n",
    "accuracy = lr.score(csr_matrix((test_x), dtype=float), test_y)\n",
    "print('LR - Test set Accuracy：'+str(round(accuracy*100, 2))+'%')\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_Y = lr.predict(csr_matrix(test_x, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58_0_12.39%\n",
      "41_1_8.76%\n",
      "64_2_13.68%\n",
      "2_3_0.43%\n",
      "76_4_16.24%\n",
      "7_5_1.5%\n",
      "220_6_47.01%\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    x = list(test_y).count(i)\n",
    "    ratio = x/ test_y.shape[0]\n",
    "    print(str(x)+ '_'+ str(i)+  '_'+ str(round(ratio*100,2))+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    x = list(PRED_Y).count(i)\n",
    "    ratio = x/ test_y.shape[0]\n",
    "    print(str(x)+ '_'+ str(i)+  '_'+ str(round(ratio*100,2))+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//LR_UP'\n",
    "pickle.dump(lr, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//aicode-master//textLearn//models//nb//nb.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pred result (单一)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "strs = '游戏没法充值'\n",
    "test_1 = [lcut(strs)]\n",
    "dict_key = list(dt.keys())\n",
    "tmp = []\n",
    "for i in test_1[0]:\n",
    "    if i in dict_key:\n",
    "        tmp.append(i)\n",
    "test_1 = [tmp]\n",
    "\n",
    "\n",
    "strs_df = pd.DataFrame({'message':[strs]})\n",
    "strs_df['char_count'] = strs_df['message'].apply(len)\n",
    "strs_df['word_count'] = strs_df['message'].apply(lambda x: len(x.split()))\n",
    "strs_df['word_density'] = strs_df['char_count'] / (strs_df['word_count']+1)\n",
    "strs_df['punctuation_count'] = strs_df['message'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "\n",
    "\n",
    "add_feature_test = strs_df[['word_count', 'word_density', 'punctuation_count']].values\n",
    "tran = tv.transform(pd.Series(strs))\n",
    "\n",
    "#tfidf = tfidf_vect.transform(pd.Series(strs))\n",
    "test_1 = vectorize(test_1, dt)\n",
    "test_1 = np.c_[np.array(test_1), np.array(add_feature(strs)).reshape((1, 2)), tran.toarray(), add_feature_test]\n",
    "\n",
    "maxminscaler = pickle.load(open('maxminscaler.pkl', 'rb'))\n",
    "test_1 = maxminscaler.transform(test_1)\n",
    "#test_1 = np.c_[np.array(test_1), np.array(add_feature(strs)).reshape((1, 2)), add_feature_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = lr.predict(test_1)\n",
    "change_dict_verse[y2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_1 = np.array(test_1)\n",
    "y = classifier.predict(test_1)\n",
    "change_dict_verse[y[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "kf = KFold(n_splits=n_fold, random_state=1234, shuffle=True)\n",
    "result_prob_list = np.zeros((test_1.shape[0], 7), dtype=float)\n",
    "for train_index, valid_index in kf.split(train_x_oversameple):\n",
    "    #print(valid_index)\n",
    "    X_train, X_test = train_x_oversameple[train_index], train_x_oversameple[valid_index]\n",
    "    y_train, y_test = train_y_oversameple[train_index], train_y_oversameple[valid_index]\n",
    "    classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "    classifier.fit(X_train, y_train)\n",
    "    result_prob_list += classifier.predict_proba(test_1)\n",
    "    del(X_train, X_test, y_train, y_test, classifier)\n",
    "change_dict_verse[np.argmax(result_prob_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in data_modify.label.unique():\n",
    "    #print(i)\n",
    "    x = list(data_modify.label).count(i)\n",
    "    ratio = x/ data_modify.shape[0]\n",
    "    print(change_dict_verse[i]+ str(round(ratio*100,2))+ '%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dict_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
