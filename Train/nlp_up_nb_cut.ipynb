{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "from jieba import lcut\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pkuseg\n",
    "from gensim import corpora\n",
    "#import langdetect \n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import numpy as np\n",
    "import string\n",
    "pd.set_option('display.max_rows', 50000)\n",
    "\n",
    "random_seed = 751031\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function zone\n",
    "def Bulid_dict(message_list):\n",
    "# 總分詞\n",
    "    ls_of_words = [lcut(text) for text in message_list]\n",
    "    # 構造詞典\n",
    "    dictionary = corpora.Dictionary(ls_of_words)\n",
    "    dt = dictionary.token2id\n",
    "    length = len(dt)\n",
    "    return dt, length\n",
    "\n",
    "def vectorize(ls_of_words, dt):\n",
    "    length = len(dt)\n",
    "    # 句向量\n",
    "    ls_of_wid = []\n",
    "    for words in ls_of_words:#ls_of_words \n",
    "        vector = [0] * length\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector[dt[word]] += 1\n",
    "            except:\n",
    "                continue\n",
    "        ls_of_wid.append(vector) \n",
    "    return ls_of_wid\n",
    "\n",
    "def add_feature(sr):\n",
    "    num_only = int(sr.isdigit())\n",
    "    abc_only = int(sr.encode( 'UTF-8').isalpha())\n",
    "    #numorabc_only = int(sr.isalnum())\n",
    "    def is_contains_chinese(strs):\n",
    "        for _char in strs:\n",
    "            if '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                return 1\n",
    "        return 0\n",
    " \n",
    "    #检验是否全是中文字符\n",
    "    def is_all_chinese(strs):\n",
    "        for _char in strs:\n",
    "            if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                return 0\n",
    "        return 1\n",
    "    #allchinese = is_all_chinese(sr)\n",
    "    #containchinese = is_all_chinese(sr)   \n",
    "    return [num_only, abc_only]\n",
    "\n",
    "\n",
    "def remove_punctuation(s): \n",
    "    punctuation = '''''：，？表情\\n!！()-[]{};:'\"\\,<>./[email protected]#$%^&*_~'''+ string.digits+string.whitespace+ string.punctuation\n",
    "    #'''''\\n!()-[]{};:'\"\\,<>./[email protected]#$%^&*_~！（）－［］｛｝；：’＂＼，＜＞．／＃＄％︿＆＊＿～'''\n",
    "    my_str = s \n",
    "    no_punct = \"\" \n",
    "    for char in my_str: \n",
    "        if char not in punctuation: \n",
    "            no_punct += char \n",
    "    return(no_punct) \n",
    "\n",
    "def change_string(articut, text):\n",
    "    if text == '':\n",
    "        tmp = ''\n",
    "    else:\n",
    "        result = articut.parse(text)\n",
    "        content = articut.getContentWordLIST(result)\n",
    "        tmp = ''\n",
    "        for sentense in content:\n",
    "            for word in sentense:\n",
    "                tmp += word[-1]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構一個總辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify = pd.read_excel(\"C://Users//user//Desktop//project//up//input//input_v5.xlsx\", index_col = None, engine='openpyxl', header=0)\n",
    "data_modify = data_modify[['message', '正确意图']]\n",
    "data_modify = data_modify[~data_modify.loc[:, '正确意图'].isna()]\n",
    "data_modify.columns = ['message', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4675, 2)\n"
     ]
    }
   ],
   "source": [
    "change_dict = {'到帐问题':0, '提款卖币问题':1, '存款买币问题':2, '银行卡问题':3, '操作问题':4, '订单取消问题':5, '非业务意图':6}\n",
    "change_dict_verse = {v: k for k, v in change_dict.items()}\n",
    "data_modify = data_modify[data_modify.label.isin(list(change_dict.keys()))].reset_index(drop=True)\n",
    "print(data_modify.shape)\n",
    "data_modify.loc[:, 'message'] = data_modify.loc[:, 'message'].apply(remove_punctuation)\n",
    "data_modify.loc[:, 'message'] = [i.upper()  for i in data_modify.message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_seg = pkuseg.pkuseg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bulid_dict(message_list):\n",
    "# 總分詞\n",
    "    ls_of_words = [pku_seg.cut(text) for text in message_list]\n",
    "    # 構造詞典\n",
    "    dictionary = corpora.Dictionary(ls_of_words)\n",
    "    dt = dictionary.token2id\n",
    "    length = len(dt)\n",
    "    return dt, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, length = Bulid_dict(data_modify.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVector = CountVectorizer()\n",
    "CountVectorizer_fit = CountVector.fit_transform(data_modify.message)\n",
    "tv = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "tv.fit(data_modify.message)\n",
    "tv_fit = tv.transform(data_modify.message)\n",
    "#pickle.dump(tv, open('tv.pkl', 'wb'))\n",
    "#tv = pickle.load(open('tv.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tv, open('tv.pkl', 'wb'))\n",
    "#tv = pickle.load(open('tv.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(data_modify.message)\n",
    "xtrain_tfidf = tfidf_vect.transform(data_modify.message)\n",
    "fuzz.token_sort_ratio(\"我想要存款\", \"我想存款\")\n",
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n",
    "train_list2 = []\n",
    "for i in train_list:\n",
    "    train_list2.append([x for x in i if x not in stop_words])\n",
    "'''\n",
    "train_list = [pku_seg.cut(text) for text in data_modify.message]#[lcut(text) for text in data_modify.message]\n",
    "train_list_vec = vectorize(train_list, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#label_dict = list(change_dict.keys())\n",
    "#data_modify['cut_count'] = data_modify['message'].apply(lambda x: len(lcut(x)))\n",
    "#data_modify['cut_density'] = data_modify['cut_count'] / (data_modify['word_count']+1)\n",
    "#data_modify['title_word_count'] = data_modify['message'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "#data_modify['v0'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[0])/100)\n",
    "#data_modify['v1'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[1])/100)\n",
    "#data_modify['v2'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[2])/100)\n",
    "#data_modify['v3'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[3])/100)\n",
    "#data_modify['v4'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[4])/100)\n",
    "#data_modify['v5'] = data_modify['message'].apply(lambda x: fuzz.token_sort_ratio(x, label_dict[5])/100)\n",
    "#data_modify['v0_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[0])/100)\n",
    "#data_modify['v1_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[1])/100)\n",
    "#data_modify['v2_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[2])/100)\n",
    "#data_modify['v3_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[3])/100)\n",
    "#data_modify['v4_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[4])/100)\n",
    "#data_modify['v5_1'] = data_modify['message'].apply(lambda x: fuzz.ratio(x, label_dict[5])/100)\n",
    "'''\n",
    "data_modify['char_count'] = data_modify['message'].apply(len)\n",
    "data_modify['word_count'] = data_modify['message'].apply(lambda x: len(x.split()))\n",
    "data_modify['word_density'] = data_modify['char_count'] / (data_modify['word_count']+1)\n",
    "data_modify['punctuation_count'] = data_modify['message'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "\n",
    "add_feature_ = data_modify[['word_count', 'word_density', 'punctuation_count']].values\n",
    "add_feature_isin = np.array([add_feature(i) for i in data_modify.message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[np.array(train_list_vec), \n",
    "                   add_feature_isin, \n",
    "                   tv_fit.toarray(),\n",
    "                   add_feature_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4675, 5488)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\project\\\\up\\\\Train'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxminscaler = MinMaxScaler()\n",
    "maxminscaler.fit(train_data)\n",
    "train_data = maxminscaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(maxminscaler, open('maxminscaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change = [change_dict[i] for i in data_modify.label]\n",
    "data_modify.loc[:, 'label'] = label_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料切分\n",
    "train_y, test_y, train_x, test_x = train_test_split(data_modify.label, \n",
    "                                                    train_data,#train_list_vec, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=random_seed)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "del(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change = [change_dict[i] for i in data_modify.label]\n",
    "data_modify.loc[:, 'label'] = label_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_oversameple = np.zeros((0,train_x.shape[1]), float)\n",
    "train_y_oversameple = np.zeros((0,), float)\n",
    "\n",
    "max_sample = train_y.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "np.random.seed(random_seed)\n",
    "for i in train_y.unique():\n",
    "    ta = list(np.random.choice(list(train_y[train_y==i].index), max_sample))\n",
    "    train_y_oversameple = np.append(train_y_oversameple, train_y[ta], axis=0)\n",
    "    train_x_oversameple = np.append(train_x_oversameple, train_x[ta], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_fold cross valid accuracy mean:0.602566224464146, max:0.6111771700356718, min:0.5945303210463734\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "kf = KFold(n_splits=n_fold, random_state=random_seed, shuffle=True)\n",
    "accuracy_list = []\n",
    "for train_index, valid_index in kf.split(train_x):\n",
    "    #print(valid_index)\n",
    "    X_train, X_test = train_x[train_index], train_x[valid_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[valid_index]\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    max_sample = X_train.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "    X_train_oversameple = np.zeros((0, X_train.shape[1]), float)\n",
    "    y_train_oversameple = np.zeros((0,), float)\n",
    "    np.random.seed(random_seed)\n",
    "    for i in range(7):\n",
    "        ta = list(np.random.choice(list(y_train[y_train==i].index), max_sample))\n",
    "        y_train_oversameple = np.append(y_train_oversameple, y_train[ta], axis=0)\n",
    "        X_train_oversameple = np.append(X_train_oversameple, X_train[ta], axis=0)    \n",
    "    classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "    classifier.fit(X_train_oversameple, y_train_oversameple)\n",
    "    # 模型測評\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    accuracy_list.append(score)\n",
    "    #del(X_train, X_test, y_train, y_test, classifier)\n",
    "    \n",
    "print('{k}_fold cross valid accuracy mean:{mean}, max:{max}, min:{min}'.format(k=n_fold, \n",
    "                                                                               mean=mean(accuracy_list),\n",
    "                                                                               max=max(accuracy_list),\n",
    "                                                                               min=min(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 貝葉斯模型訓練\n",
    "classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "classifier.fit(train_x_oversameple, train_y_oversameple)\n",
    "\n",
    "# 模型測評\n",
    "score = classifier.score(test_x, test_y)\n",
    "print('Test set Accuracy：'+ str(round(score*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//aicode-master//textLearn//models//nb//nb.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "5_fold cross valid accuracy mean:0.8202959377056496, max:0.8396674584323041, min:0.8061831153388823\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "random_seed = 751031\n",
    "kf = KFold(n_splits=n_fold, random_state=random_seed, shuffle=True)\n",
    "accuracy_list = []\n",
    "for train_index, valid_index in kf.split(train_x):\n",
    "    #print(valid_index)\n",
    "    X_train, X_test = train_x[train_index], train_x[valid_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[valid_index]\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    #for i in range(7):\n",
    "        #x = list(y_test).count(i)\n",
    "        #ratio = x/ y_test.shape[0]\n",
    "        #print(change_dict_verse[i]+ str(round(ratio*100,2))+ '%')\n",
    "    #print(\"----------------------------\")\n",
    "    max_sample = X_train.shape[0]#Counter(train_y).most_common(1)[0][1]\n",
    "    X_train_oversameple = np.zeros((0, X_train.shape[1]), float)\n",
    "    y_train_oversameple = np.zeros((0,), float)\n",
    "    np.random.seed(random_seed)\n",
    "    for i in range(7):\n",
    "        ta = list(np.random.choice(list(y_train[y_train==i].index), max_sample))\n",
    "        y_train_oversameple = np.append(y_train_oversameple, y_train[ta], axis=0)\n",
    "        X_train_oversameple = np.append(X_train_oversameple, X_train[ta], axis=0)    \n",
    "    lr = LogisticRegression(solver='saga',\n",
    "                            multi_class='multinomial',\n",
    "                            C=1,\n",
    "                            penalty='l1',\n",
    "                            fit_intercept=True,\n",
    "                            max_iter=5000,\n",
    "                            random_state=4180,\n",
    "                            class_weight='balanced',\n",
    "                            n_jobs=-1)\n",
    "    lr.fit(csr_matrix((X_train_oversameple), dtype=float), \n",
    "           y_train_oversameple)\n",
    "    # 模型測評\n",
    "    score = lr.score(csr_matrix((X_test), dtype=float), \n",
    "                     y_test)\n",
    "    accuracy_list.append(score)\n",
    "    print('-----')\n",
    "    #del(X_train, X_test, y_train, y_test, classifier)\n",
    "    \n",
    "print('{k}_fold cross valid accuracy mean:{mean}, max:{max}, min:{min}'.format(k=n_fold, \n",
    "                                                                               mean=mean(accuracy_list),\n",
    "                                                                               max=max(accuracy_list),\n",
    "                                                                               min=min(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oversameple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "lr = LogisticRegression(solver='saga',\n",
    "                        multi_class='multinomial',\n",
    "                        C=1,\n",
    "                        penalty='l1',\n",
    "                        fit_intercept=True,\n",
    "                        max_iter=5000,\n",
    "                        random_state=4180,\n",
    "                        class_weight='balanced',\n",
    "                        n_jobs=-1)\n",
    "lr.fit(csr_matrix((train_x_oversameple), dtype=float),\n",
    "       train_y_oversameple)\n",
    "accuracy = lr.score(csr_matrix((test_x), dtype=float), \n",
    "                    test_y)\n",
    "print('LR - Test set Accuracy：'+str(round(accuracy*100, 2))+'%')\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_Y = lr.predict(csr_matrix(test_x, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    x = list(test_y).count(i)\n",
    "    ratio = x/ test_y.shape[0]\n",
    "    print(str(x)+ '_'+ str(i)+  '_'+ str(round(ratio*100,2))+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    x = list(PRED_Y).count(i)\n",
    "    ratio = x/ test_y.shape[0]\n",
    "    print(str(x)+ '_'+ str(i)+  '_'+ str(round(ratio*100,2))+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//LR_UP'\n",
    "pickle.dump(lr, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#save model and load the model\n",
    "filename = 'C://Users//user//Desktop//aicode-master//textLearn//models//nb//nb.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "#classifier = pickle.load(open(filename, 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pred result (单一)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "strs = '游戏没法充值'\n",
    "test_1 = [lcut(strs)]\n",
    "dict_key = list(dt.keys())\n",
    "tmp = []\n",
    "for i in test_1[0]:\n",
    "    if i in dict_key:\n",
    "        tmp.append(i)\n",
    "test_1 = [tmp]\n",
    "\n",
    "\n",
    "strs_df = pd.DataFrame({'message':[strs]})\n",
    "strs_df['char_count'] = strs_df['message'].apply(len)\n",
    "strs_df['word_count'] = strs_df['message'].apply(lambda x: len(x.split()))\n",
    "strs_df['word_density'] = strs_df['char_count'] / (strs_df['word_count']+1)\n",
    "strs_df['punctuation_count'] = strs_df['message'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "\n",
    "\n",
    "add_feature_test = strs_df[['word_count', 'word_density', 'punctuation_count']].values\n",
    "tran = tv.transform(pd.Series(strs))\n",
    "\n",
    "#tfidf = tfidf_vect.transform(pd.Series(strs))\n",
    "test_1 = vectorize(test_1, dt)\n",
    "test_1 = np.c_[np.array(test_1), np.array(add_feature(strs)).reshape((1, 2)), tran.toarray(), add_feature_test]\n",
    "\n",
    "maxminscaler = pickle.load(open('maxminscaler.pkl', 'rb'))\n",
    "test_1 = maxminscaler.transform(test_1)\n",
    "#test_1 = np.c_[np.array(test_1), np.array(add_feature(strs)).reshape((1, 2)), add_feature_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = lr.predict(test_1)\n",
    "change_dict_verse[y2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_1 = np.array(test_1)\n",
    "y = classifier.predict(test_1)\n",
    "change_dict_verse[y[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "kf = KFold(n_splits=n_fold, random_state=1234, shuffle=True)\n",
    "result_prob_list = np.zeros((test_1.shape[0], 7), dtype=float)\n",
    "for train_index, valid_index in kf.split(train_x_oversameple):\n",
    "    #print(valid_index)\n",
    "    X_train, X_test = train_x_oversameple[train_index], train_x_oversameple[valid_index]\n",
    "    y_train, y_test = train_y_oversameple[train_index], train_y_oversameple[valid_index]\n",
    "    classifier = MultinomialNB()  # 樸素貝葉斯分類器\n",
    "    classifier.fit(X_train, y_train)\n",
    "    result_prob_list += classifier.predict_proba(test_1)\n",
    "    del(X_train, X_test, y_train, y_test, classifier)\n",
    "change_dict_verse[np.argmax(result_prob_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in data_modify.label.unique():\n",
    "    #print(i)\n",
    "    x = list(data_modify.label).count(i)\n",
    "    ratio = x/ data_modify.shape[0]\n",
    "    print(change_dict_verse[i]+ str(round(ratio*100,2))+ '%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dict_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'usdt我要摸報'.upper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
